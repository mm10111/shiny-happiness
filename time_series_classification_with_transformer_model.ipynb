{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w_8AV_eyYzDb"
      },
      "outputs": [],
      "source": [
        "#link to tutorial: https://keras.io/examples/timeseries/timeseries_classification_transformer/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np "
      ],
      "metadata": {
        "id": "NL-dwogQZJqa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a file read function \n",
        "def read_data_file(filename):\n",
        "  dataset = np.loadtxt(filename, delimiter=\"\\t\")\n",
        "  y = dataset[:, 0]\n",
        "  x = dataset[:, 1:]\n",
        "  return x, y.astype(int)"
      ],
      "metadata": {
        "id": "kHDS8ZHFZDYY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\""
      ],
      "metadata": {
        "id": "s-7zWYhaZQJL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the dataset and split it into training data and test dataset\n",
        "x_training_set, y_training_set = read_data_file(dataset_url + \"FordA_TRAIN.tsv\")\n",
        "x_test_set, y_test_set = read_data_file(dataset_url + \"FordA_TEST.tsv\")"
      ],
      "metadata": {
        "id": "HD8EIyuXZrIc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validating the data was read properly\n",
        "x_training_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmCmpP97ZyLc",
        "outputId": "50f980ce-0b3d-4a41-c6e4-458949d90ee5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.79717168, -0.66439208, -0.37301463, ..., -0.66439208,\n",
              "        -1.0737958 , -1.5643427 ],\n",
              "       [ 0.80485472,  0.63462859,  0.37347448, ..., -0.71488505,\n",
              "        -0.56044294, -0.31908642],\n",
              "       [ 0.7279851 ,  0.11128392, -0.49912439, ...,  0.39446303,\n",
              "         0.33940042,  0.25539062],\n",
              "       ...,\n",
              "       [-0.57005428, -0.33316523, -0.29351853, ..., -1.3937145 ,\n",
              "        -0.94273327, -0.27072168],\n",
              "       [ 2.0067321 ,  2.0791499 ,  2.0220362 , ..., -0.43214504,\n",
              "        -0.44123126, -0.28070891],\n",
              "       [-0.12524091, -0.32536268, -0.48823697, ...,  0.55576053,\n",
              "         0.57445102,  0.57311598]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_training_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUzz49VTaaGI",
        "outputId": "6d35d1e7-3667-4eb0-ed92-092c595f0bd6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  1, -1, ..., -1,  1, -1])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4rhERKTacjP",
        "outputId": "77c90347-b449-426b-fe88-62eac37908b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.14040239,  0.17164128,  0.30204415, ..., -0.69040244,\n",
              "        -0.97659635, -0.79426313],\n",
              "       [ 0.33403756,  0.32225332,  0.45384397, ..., -1.0417721 ,\n",
              "        -1.1596145 , -1.3756589 ],\n",
              "       [ 0.71668608,  0.74436655,  0.72591291, ..., -3.6752806 ,\n",
              "        -4.1366217 , -4.3396117 ],\n",
              "       ...,\n",
              "       [ 0.71008362,  0.59397882,  0.3818858 , ..., -0.12655282,\n",
              "        -0.11782239, -0.18909413],\n",
              "       [ 0.00684706, -0.14062427, -0.27059412, ..., -1.0007084 ,\n",
              "        -1.0841075 , -1.109963  ],\n",
              "       [-0.54135529, -0.24172258,  0.10074086, ..., -0.09362504,\n",
              "        -0.90080431, -1.778341  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLg-eIhYafEx",
        "outputId": "f9fbc08d-2dcf-4240-8399-1771cdf6e725"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, ...,  1,  1,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# next step is to reshape the training and test set into vectors \n",
        "x_training_set = x_training_set.reshape((x_training_set.shape[0], x_training_set.shape[1], 1))\n",
        "x_test_set = x_test_set.reshape((x_test_set.shape[0], x_test_set.shape[1], 1))"
      ],
      "metadata": {
        "id": "orinoQ1wagv6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the number of classes in the dataset\n",
        "number_of_classes = len(np.unique(y_training_set))"
      ],
      "metadata": {
        "id": "5-pLYarOis4p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffling the training dataset \n",
        "shuffle = np.random.permutation(len(x_training_set))\n",
        "x_training_set = x_training_set[shuffle]\n",
        "y_training_set = y_training_set[shuffle]\n",
        "\n",
        "y_training_set[y_training_set == -1] = 0\n",
        "y_test_set[y_test_set == -1] = 0"
      ],
      "metadata": {
        "id": "IAMSDBpojBpK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the data is ready for training \n",
        "# model development time\n",
        "# model input is a tensor of shape (batch_size, sequence_length, features) where sequence_length is the number of time steps and features is each input timeseries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers "
      ],
      "metadata": {
        "id": "DLhnFOuxi6Kd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the projection layers are implemented using keras.layers.Conv1D\n",
        "\n",
        "# creating the model function for encoding data\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "  # attention and normalization of the data\n",
        "  x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "  x = layers.Dropout(dropout)(x)\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "  res = x + inputs\n",
        "\n",
        "  # feed forward component\n",
        "  x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "  x = layers.Dropout(dropout)(x)\n",
        "  x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "  return x + res"
      ],
      "metadata": {
        "id": "f2sHtAekkCiU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to build the model, multiple transformer encoders can be stacked together and as the last layer add a multi-layer perceptron classifier.\n",
        "# the output tensor from the transformer encoder has to be reduced to a vector of features for each data point\n",
        "# you can achieve that by using a pooling layer."
      ],
      "metadata": {
        "id": "l3Oq0KZhoor3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "  model_inputs = keras.Input(shape=input_shape)\n",
        "  x = model_inputs\n",
        "  for _ in range(num_transformer_blocks):\n",
        "    x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "  \n",
        "  x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "  for dim in mlp_units:\n",
        "    x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(mlp_dropout)(x)\n",
        "  \n",
        "  outputs = layers.Dense(number_of_classes, activation=\"softmax\")(x)\n",
        "  return keras.Model(model_inputs, outputs)\n"
      ],
      "metadata": {
        "id": "WEtCX-o8p3NU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and evaluate model performance\n",
        "input_shape = x_training_set.shape[1:]\n",
        "\n",
        "transformer_model = build_model(input_shape, head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], mlp_dropout=0.4, dropout=0.25)\n",
        "\n",
        "transformer_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=1e-4), metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "transformer_model.summary()\n",
        "\n",
        "# stop the model training early if the model has stopped improving \n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "transformer_model.fit(x_training_set, y_training_set, validation_split=0.2, epochs=200, batch_size=64, callbacks=callbacks)\n",
        "\n",
        "transformer_model.evaluate(x_test_set, y_test_set, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKXPvguNrZSY",
        "outputId": "765236ff-6ff9-4b3a-f665-cce665d5e637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 500, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 500, 1)      7169        ['input_2[0][0]',                \n",
            " eadAttention)                                                    'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 500, 1)       0           ['multi_head_attention_4[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 500, 1)      2           ['dropout_9[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 500, 1)      0           ['layer_normalization_8[0][0]',  \n",
            " mbda)                                                            'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 500, 4)       8           ['tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 500, 4)       0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 500, 1)       5           ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 500, 1)      2           ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 500, 1)      0           ['layer_normalization_9[0][0]',  \n",
            " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 500, 1)      7169        ['tf.__operators__.add_9[0][0]', \n",
            " eadAttention)                                                    'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 500, 1)       0           ['multi_head_attention_5[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 500, 1)      2           ['dropout_11[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 500, 1)      0           ['layer_normalization_10[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 500, 4)       8           ['tf.__operators__.add_10[0][0]']\n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 500, 4)       0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 500, 1)       5           ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 500, 1)      2           ['conv1d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 500, 1)      0           ['layer_normalization_11[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 500, 1)      7169        ['tf.__operators__.add_11[0][0]',\n",
            " eadAttention)                                                    'tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 500, 1)       0           ['multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 500, 1)      2           ['dropout_13[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 500, 1)      0           ['layer_normalization_12[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 500, 4)       8           ['tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 500, 4)       0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 500, 1)       5           ['dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 500, 1)      2           ['conv1d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 500, 1)      0           ['layer_normalization_13[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 500, 1)      7169        ['tf.__operators__.add_13[0][0]',\n",
            " eadAttention)                                                    'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 500, 1)       0           ['multi_head_attention_7[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 500, 1)      2           ['dropout_15[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 500, 1)      0           ['layer_normalization_14[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 500, 4)       8           ['tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 500, 4)       0           ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 500, 1)       5           ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 500, 1)      2           ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 500, 1)      0           ['layer_normalization_15[0][0]', \n",
            " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 500)         0           ['tf.__operators__.add_15[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 128)          64128       ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 128)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 2)            258         ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 93,130\n",
            "Trainable params: 93,130\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "45/45 [==============================] - 1592s 35s/step - loss: 0.9653 - sparse_categorical_accuracy: 0.5128 - val_loss: 0.7213 - val_sparse_categorical_accuracy: 0.5687\n",
            "Epoch 2/200\n",
            "45/45 [==============================] - 1587s 35s/step - loss: 0.8488 - sparse_categorical_accuracy: 0.5500 - val_loss: 0.6487 - val_sparse_categorical_accuracy: 0.6255\n",
            "Epoch 3/200\n",
            "45/45 [==============================] - 1589s 35s/step - loss: 0.7853 - sparse_categorical_accuracy: 0.5833 - val_loss: 0.6083 - val_sparse_categorical_accuracy: 0.6560\n",
            "Epoch 4/200\n",
            "45/45 [==============================] - 1574s 35s/step - loss: 0.7320 - sparse_categorical_accuracy: 0.6087 - val_loss: 0.5796 - val_sparse_categorical_accuracy: 0.6879\n",
            "Epoch 5/200\n",
            "45/45 [==============================] - 1595s 36s/step - loss: 0.6847 - sparse_categorical_accuracy: 0.6403 - val_loss: 0.5595 - val_sparse_categorical_accuracy: 0.7018\n",
            "Epoch 6/200\n",
            "45/45 [==============================] - 1590s 35s/step - loss: 0.6777 - sparse_categorical_accuracy: 0.6517 - val_loss: 0.5450 - val_sparse_categorical_accuracy: 0.7198\n",
            "Epoch 7/200\n",
            "45/45 [==============================] - 1598s 36s/step - loss: 0.6314 - sparse_categorical_accuracy: 0.6705 - val_loss: 0.5345 - val_sparse_categorical_accuracy: 0.7295\n",
            "Epoch 8/200\n",
            "45/45 [==============================] - 1585s 35s/step - loss: 0.6066 - sparse_categorical_accuracy: 0.6799 - val_loss: 0.5229 - val_sparse_categorical_accuracy: 0.7420\n",
            "Epoch 9/200\n",
            "45/45 [==============================] - 1594s 36s/step - loss: 0.5942 - sparse_categorical_accuracy: 0.6944 - val_loss: 0.5141 - val_sparse_categorical_accuracy: 0.7434\n",
            "Epoch 10/200\n",
            "45/45 [==============================] - 1587s 35s/step - loss: 0.5850 - sparse_categorical_accuracy: 0.7101 - val_loss: 0.5075 - val_sparse_categorical_accuracy: 0.7448\n",
            "Epoch 11/200\n",
            "45/45 [==============================] - 1599s 36s/step - loss: 0.5640 - sparse_categorical_accuracy: 0.7104 - val_loss: 0.5002 - val_sparse_categorical_accuracy: 0.7503\n",
            "Epoch 12/200\n",
            "45/45 [==============================] - 1604s 36s/step - loss: 0.5551 - sparse_categorical_accuracy: 0.7222 - val_loss: 0.4897 - val_sparse_categorical_accuracy: 0.7670\n",
            "Epoch 13/200\n",
            "45/45 [==============================] - 1570s 35s/step - loss: 0.5457 - sparse_categorical_accuracy: 0.7299 - val_loss: 0.4832 - val_sparse_categorical_accuracy: 0.7656\n",
            "Epoch 14/200\n",
            "45/45 [==============================] - 1599s 36s/step - loss: 0.5210 - sparse_categorical_accuracy: 0.7427 - val_loss: 0.4752 - val_sparse_categorical_accuracy: 0.7698\n",
            "Epoch 15/200\n",
            "28/45 [=================>............] - ETA: 9:12 - loss: 0.5230 - sparse_categorical_accuracy: 0.7400"
          ]
        }
      ]
    }
  ]
}